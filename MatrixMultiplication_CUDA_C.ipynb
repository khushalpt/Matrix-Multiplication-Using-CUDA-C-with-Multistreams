{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1106937_w4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoTw_v7BKFJj"
      },
      "source": [
        "**Machine Learning Assignment 4**\n",
        "\n",
        "Content:\n",
        "\n",
        "1.   Cuda Check\n",
        "2.   Install Plug in to let write and execute cuda code in google colab\n",
        "3.   Cuda Code\n",
        "4.   Cuda Output\n",
        "5.   **Python Codes**\n",
        "\n",
        "    5.a. Python code for *Assignment 3 part 2*\n",
        "\n",
        "    5.b. Python Code for *Assignment 4* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1W2WTJkI1GT"
      },
      "source": [
        "1. Check Cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV5d0gr0amG_",
        "outputId": "f49d54d6-4114-46cd-dd5d-4bf0c2aefaec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check cuda\n",
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov 17 17:08:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaC9mSlwI6rR"
      },
      "source": [
        "2. Install Plug in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03GZgd9XaRlY",
        "outputId": "6f8b327f-b5be-4600-c0b3-44b8d100d595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install nvcc plugin and load it.\n",
        "\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-onjuy7tf\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-onjuy7tf\n",
            "Requirement already satisfied (use --upgrade to upgrade): NVCCPlugin==0.0.2 from git+git://github.com/andreinechaev/nvcc4jupyter.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=c3956477be247d614eb3b65fe7c92e2864b764d95af1cd68e65f6b8148ca93dd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6nw124zw/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb_-CU4KI9PP"
      },
      "source": [
        "3. Cuda Code for Assignment 4 with improvement from the previous code (Assignment 3 Part 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIQcShTzaTNq",
        "outputId": "97c817b3-028d-4220-8cdf-3be038419054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%cuda --name matrix_cublas_v2.cu \n",
        "// !nvcc -o /content/src/matrix_cublas_v2 /content/src/matrix_cublas_v2.cu -lcublas -lcurand\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <time.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"curand.h\"\n",
        "\n",
        "\n",
        "#define CUDA_CALL(x) do { if ((x) != cudaSuccess) { \\\n",
        "    printf(\"Error at %s:%d\\n\", __FILE__, __LINE__);\\\n",
        "    return EXIT_FAILURE; }} while(0)\n",
        "\n",
        "\n",
        "cudaError_t checkCuda();\n",
        "\n",
        "\n",
        "/**\n",
        " * Run the matrix multiplication using CUDA cuBlas\n",
        " */\n",
        "int matrixMultiply(dim3& dimsA, dim3& dimsB, int N)\n",
        "{\n",
        "    // Allocate host and device memory for matrices A, B and C\n",
        "    unsigned int size_A = dimsA.x * dimsA.y;\n",
        "    unsigned int mem_size_A = sizeof(float) * size_A;\n",
        "    float* h_A = NULL;\n",
        "    CUDA_CALL(cudaHostAlloc(&h_A, mem_size_A, cudaHostAllocDefault));\n",
        "    float* d_A = NULL;\n",
        "    CUDA_CALL(cudaMalloc(&d_A, mem_size_A));\n",
        "\n",
        "    unsigned int size_B = dimsB.x * dimsB.y;\n",
        "    unsigned int mem_size_B = sizeof(float) * size_B;\n",
        "    float* h_B = NULL;\n",
        "    CUDA_CALL(cudaHostAlloc(&h_B, mem_size_B, cudaHostAllocDefault));\n",
        "    float* d_B = NULL;\n",
        "    CUDA_CALL(cudaMalloc(&d_B, mem_size_B));\n",
        "\n",
        "    dim3 dimsC(dimsB.x, dimsA.y, 1);\n",
        "    unsigned int size_C = dimsC.x * dimsC.y;\n",
        "    unsigned int mem_size_C = sizeof(float) * size_C;\n",
        "    float *h_C = NULL;\n",
        "    CUDA_CALL(cudaHostAlloc(&h_C, mem_size_C, cudaHostAllocDefault));\n",
        "    float *d_C = NULL;\n",
        "    CUDA_CALL(cudaMalloc(&d_C, mem_size_C));\n",
        "\n",
        "    // initiate the random generator on GPU\n",
        "    curandGenerator_t generator;\n",
        "    CUDA_CALL(curandCreateGenerator(&generator, CURAND_RNG_PSEUDO_XORWOW));\n",
        "    CUDA_CALL(curandSetPseudoRandomGeneratorSeed(generator, (int)time(NULL)));\n",
        "\n",
        "    // create cuda stream\n",
        "    const int NUM_STREAMS = 2;\n",
        "    cudaStream_t streams[NUM_STREAMS];\n",
        "    for (int i = 0; i < NUM_STREAMS; ++i)\n",
        "    {\n",
        "        CUDA_CALL(cudaStreamCreate(&streams[i]));\n",
        "    }\n",
        "\n",
        "    // Allocate CUDA events that we'll use for timing\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CALL(cudaEventCreate(&start));\n",
        "    CUDA_CALL(cudaEventCreate(&stop));\n",
        "\n",
        "    // create cublas handle\n",
        "    const float alpha = 1.0f;\n",
        "    const float beta  = 0.0f;\n",
        "    cublasHandle_t handle;\n",
        "    CUDA_CALL(cublasCreate(&handle));\n",
        "\n",
        "    // Starting\n",
        "    CUDA_CALL(cudaEventRecord(start, NULL));\n",
        "    for(int i = 0; i < N; ++i)\n",
        "    {\n",
        "        // Initialising streams\n",
        "        int stream_index = i % NUM_STREAMS;\n",
        "        cudaStream_t stream = streams[stream_index];\n",
        "\n",
        "        // generate device matrixes d_A and d_B directly\n",
        "        CUDA_CALL(curandGenerateUniform(generator, d_A, size_A));\n",
        "        CUDA_CALL(curandGenerateUniform(generator, d_B, size_B));\n",
        "\n",
        "        // caculate matrix C = A * B by using cuBlas\n",
        "        cublasSetStream(handle, stream);\n",
        "        CUDA_CALL(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, dimsB.x, dimsA.y, dimsA.x, &alpha, \n",
        "            d_B, dimsB.x, d_A, dimsA.x, &beta, d_C, dimsB.x));\n",
        "\n",
        "        // copy result from device to host\n",
        "        CUDA_CALL(cudaMemcpyAsync(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost, stream));\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; ++i)\n",
        "    {\n",
        "        CUDA_CALL(cudaStreamSynchronize(streams[i]));\n",
        "    }\n",
        "\n",
        "    // Record the stop event\n",
        "    CUDA_CALL(cudaEventRecord(stop, NULL));\n",
        "    CUDA_CALL(cudaEventSynchronize(stop));\n",
        "\n",
        "    float msec_TotalMatrixMul = 0.0f;\n",
        "    CUDA_CALL(cudaEventElapsedTime(&msec_TotalMatrixMul, start, stop));\n",
        "\n",
        "    // Compute and print the performance\n",
        "    float msec_MatrixMul = msec_TotalMatrixMul / N;\n",
        "    double flops_MatrixMul = 2.0 * (double)dimsA.x * (double)dimsA.y * (double)dimsB.x;\n",
        "    double flopsGiga = (flops_MatrixMul * 1.0e-9f) / (msec_MatrixMul / 1000.0f);\n",
        "    printf(\"Performance= %.2f GFlop/s, AVGTime= %.3f msec, TotalTime=%.3f msc \\n\",\n",
        "        flopsGiga, msec_MatrixMul, msec_TotalMatrixMul);\n",
        " \n",
        "    // Memory Clean up\n",
        "    cudaFreeHost(h_A);\n",
        "    cudaFreeHost(h_B);\n",
        "    cudaFreeHost(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    for (int i = 0; i < NUM_STREAMS; ++i)\n",
        "    {\n",
        "        CUDA_CALL(cudaStreamDestroy(streams[i]));\n",
        "    }\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "} \n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    if(checkCuda() != cudaSuccess)\n",
        "    {\n",
        "        return 0;\n",
        "    }\n",
        " \n",
        "    // condition i) A(500*500), B(500*400), N=100\n",
        "    printf(\"Performing condition i: A(500*500), B(500*400), N=100\\n\");\n",
        "    int N1 = 100;\n",
        "    dim3 dimsA1(500, 500, 1);\n",
        "    dim3 dimsB1(400, 500, 1);\n",
        "    matrixMultiply(dimsA1, dimsB1, N1);\n",
        "    printf(\"\\n\");\n",
        " \n",
        "    // condition ii) A(50*20), B(20*50), N=5000\n",
        "    printf(\"Performing condition ii: A(50*20), B(20*50), N=5000\\n\");\n",
        "    int N2 = 5000;\n",
        "    dim3 dimsA2(20, 50, 1);\n",
        "    dim3 dimsB2(50, 20, 1);\n",
        "    matrixMultiply(dimsA2, dimsB2, N2);\n",
        "    printf(\"\\n\");\n",
        " \n",
        "    // condition iii) A(6*4000), B(4000*9), N=1000\n",
        "    printf(\"Performing condition iii: A(6*4000), B(4000*9), N=1000\\n\");\n",
        "    int N3 = 1000;\n",
        "    dim3 dimsA3(4000, 6, 1);\n",
        "    dim3 dimsB3(9, 4000, 1);\n",
        "    matrixMultiply(dimsA3, dimsB3, N3);\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "cudaError_t checkCuda()\n",
        "{\n",
        "    printf(\"Checking CUDA...\\n\");\n",
        "\n",
        "    int devID = 0;\n",
        "    cudaError_t error;\n",
        "    cudaDeviceProp deviceProp;\n",
        "    error = cudaGetDevice(&devID);\n",
        "\n",
        "    if (error != cudaSuccess)\n",
        "    {\n",
        "        printf(\"cudaGetDevice returned error %s (code %d), line(%d)\\n\", cudaGetErrorString(error), error, __LINE__);\n",
        "        return error;\n",
        "    }\n",
        "\n",
        "    error = cudaGetDeviceProperties(&deviceProp, devID);\n",
        "\n",
        "    if (deviceProp.computeMode == cudaComputeModeProhibited)\n",
        "    {\n",
        "        fprintf(stderr, \"Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\\n\");\n",
        "        exit(EXIT_SUCCESS);\n",
        "    }\n",
        "\n",
        "    if (error != cudaSuccess)\n",
        "    {\n",
        "        printf(\"cudaGetDeviceProperties returned error %s (code %d), line(%d)\\n\", cudaGetErrorString(error), error, __LINE__);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"GPU Device %d: \\\"%s\\\" with compute capability %d.%d\\n\\n\", devID, deviceProp.name, deviceProp.major, deviceProp.minor);\n",
        "    }\n",
        "\n",
        "    return error;\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/matrix_cublas_v2.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VyweDqaaVxL",
        "outputId": "a2b25f1e-dc7b-4f51-f981-b99d19d999cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc -o /content/src/matrix_cublas_v2 /content/src/matrix_cublas_v2.cu -lcublas -lcurand"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[K/content/src/matrix_cublas_v2.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint matrixMultiply(dim3&, dim3&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/src/matrix_cublas_v2.cu:48:87:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[KcurandStatus_t {aka enum curandStatus}\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "     CUDA_CALL(curandCreateGenerator(&generator, CURAND_RNG_PSEUDO_XORWOW));\n",
            "                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/src/matrix_cublas_v2.cu:49:92:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[KcurandStatus_t {aka enum curandStatus}\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "     CUDA_CALL(curandSetPseudoRandomGeneratorSeed(generator, (int)time(NULL)));\n",
            "                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/src/matrix_cublas_v2.cu:68:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[Kenum cublasStatus_t\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "     CUDA_CALL(cublasCreate(&handle));\n",
            "                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/src/matrix_cublas_v2.cu:79:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[KcurandStatus_t {aka enum curandStatus}\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "         CUDA_CALL(curandGenerateUniform(generator, d_A, size_A));\n",
            "                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/src/matrix_cublas_v2.cu:80:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[KcurandStatus_t {aka enum curandStatus}\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "         CUDA_CALL(curandGenerateUniform(generator, d_B, size_B));\n",
            "                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/src/matrix_cublas_v2.cu:84:160:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[Kenum cublasStatus_t\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "         CUDA_CALL(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, dimsB.x, dimsA.y, dimsA.x, &alpha,\n",
            "                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRwYnUIEJHdI"
      },
      "source": [
        "4. Cuda Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrGiZvVnaXot",
        "outputId": "a2b8fcf2-6431-417a-a076-6ab90455c3c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!/content/src/matrix_cublas_v2"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking CUDA...\n",
            "GPU Device 0: \"Tesla T4\" with compute capability 7.5\n",
            "\n",
            "Performing condition i: A(500*500), B(500*400), N=100\n",
            "Performance= 755.39 GFlop/s, AVGTime= 0.265 msec, TotalTime=26.477 msc \n",
            "\n",
            "Performing condition ii: A(50*20), B(20*50), N=5000\n",
            "Performance= 3.27 GFlop/s, AVGTime= 0.031 msec, TotalTime=152.923 msc \n",
            "\n",
            "Performing condition iii: A(6*4000), B(4000*9), N=1000\n",
            "Performance= 9.77 GFlop/s, AVGTime= 0.044 msec, TotalTime=44.204 msc \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Rd6MwHJX56"
      },
      "source": [
        "**Python Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F9z4opUJOci"
      },
      "source": [
        "5. a. Python Version for Assignment 3 Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaPIZCUvJf65",
        "outputId": "ff9631e7-e427-44e1-f021-e43ed1a1e01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Python code for Assignment 3 Part 2\n",
        "\n",
        "import numpy as np \n",
        "from timeit import default_timer as timer\n",
        "import threading\n",
        "\n",
        "# utilising threads\n",
        "class MatrixMulThread(threading.Thread):\n",
        "    \n",
        "    def __init__(self, shape_A, B, sub_N):\n",
        "        super(MatrixMulThread, self).__init__()\n",
        "        self.shape_A = shape_A\n",
        "        self.B = B\n",
        "        self.sub_N = sub_N\n",
        "\n",
        "        self.Cs = []\n",
        "\n",
        "    def run(self) -> None:\n",
        "        for _ in range(self.sub_N):\n",
        "            A = np.random.rand(self.shape_A[0], self.shape_A[1])\n",
        "            C = np.matmul(A, self.B)\n",
        "            self.Cs.append(C)\n",
        "\n",
        "# function to perform multiplication\n",
        "def matrix_multiply(shape1, shape2, N):\n",
        "    height_A, width_A = shape1\n",
        "    height_B, width_B = shape2\n",
        "    B = np.random.rand(height_B, width_B)\n",
        "    \n",
        "    num_threads = 10\n",
        "    threads = []\n",
        "    sub_N = N // num_threads\n",
        "    sub_N_last = N % num_threads\n",
        "    \n",
        "    start = timer()\n",
        "    for i in range(num_threads + 1):\n",
        "        thread_N = sub_N\n",
        "        if sub_N_last > 0 and i == num_threads:\n",
        "            thread_N = sub_N_last\n",
        "\n",
        "        thread = MatrixMulThread(shape1, B, thread_N)\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "\n",
        "    # waiting all thread finish\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "    \n",
        "    # get result from each thread\n",
        "    for thread in threads:\n",
        "        for C in thread.Cs:\n",
        "            ret_C = C\n",
        "            # print(C.shape)\n",
        "\n",
        "    end = timer()\n",
        "\n",
        "    msec_TotalMatrixMul = (end - start) * 1000\n",
        "    msec_MatrixMul = msec_TotalMatrixMul / N\n",
        "    flopsPerMatrixMul = 2.0 * width_A * height_A * width_B\n",
        "    flopsGiga = (flopsPerMatrixMul * 1.0e-9) / (msec_MatrixMul / 1000.0)\n",
        "    print(\"Performance= {:.2f} GFlop/s, AVGTime= {:.3f} msec, TotalTime= {:.3f} msc \\n\"\n",
        "            .format(flopsGiga, msec_MatrixMul, msec_TotalMatrixMul))\n",
        "\n",
        "def main():\n",
        "    # condition i) A(500*500), B(500*400), N=100\n",
        "    print(\"Performing condition i: A(500*500), B(500*400), N=100\")\n",
        "    matrix_multiply((500, 500), (500, 400), 100)\n",
        "\n",
        "    # condition ii) A(50*20), B(20*50), N=5000\n",
        "    print(\"Performing condition ii: A(50*20), B(20*50), N=5000\")\n",
        "    matrix_multiply((50, 20), (20, 50), 5000)\n",
        "\n",
        "    # condition iii) A(6*4000), B(4000*9), N=1000\n",
        "    print(\"Performing condition iii: A(6*4000), B(4000*9), N=1000\")\n",
        "    matrix_multiply((6, 4000), (4000, 9), 1000)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing condition i: A(500*500), B(500*400), N=100\n",
            "Performance= 20.22 GFlop/s, AVGTime= 9.891 msec, TotalTime= 989.062 msc \n",
            "\n",
            "Performing condition ii: A(50*20), B(20*50), N=5000\n",
            "Performance= 2.21 GFlop/s, AVGTime= 0.045 msec, TotalTime= 226.731 msc \n",
            "\n",
            "Performing condition iii: A(6*4000), B(4000*9), N=1000\n",
            "Performance= 1.20 GFlop/s, AVGTime= 0.360 msec, TotalTime= 360.352 msc \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtQ_RMA0JxFQ"
      },
      "source": [
        "5. b. Python code for assignment 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwegnHwxlA3o",
        "outputId": "07df5b39-9a90-429c-921e-eb37bbfbfd6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Python code for Assignment 4\n",
        "\n",
        "\n",
        "import numpy as np \n",
        "from timeit import default_timer as timer\n",
        "import threading\n",
        "\n",
        "\n",
        "class MatrixMulThread(threading.Thread):\n",
        "    \n",
        "    def __init__(self, shape_A, shape_B, sub_N):\n",
        "        super(MatrixMulThread, self).__init__()\n",
        "        self.shape_A = shape_A\n",
        "        self.shape_B = shape_B\n",
        "        self.sub_N = sub_N\n",
        "\n",
        "        self.Cs = []\n",
        "\n",
        "    def run(self) -> None:\n",
        "        for _ in range(self.sub_N):\n",
        "            A = np.random.rand(self.shape_A[0], self.shape_A[1])\n",
        "            B = np.random.rand(self.shape_B[0], self.shape_B[1])\n",
        "            C = np.matmul(A, B)\n",
        "            self.Cs.append(C)\n",
        "\n",
        "\n",
        "def matrix_multiply(shape1, shape2, N):\n",
        "    height_A, width_A = shape1\n",
        "    height_B, width_B = shape2\n",
        "    B = np.random.rand(height_B, width_B)\n",
        "    \n",
        "    num_threads = 10\n",
        "    threads = []\n",
        "    sub_N = N // num_threads\n",
        "    sub_N_last = N % num_threads\n",
        "    \n",
        "    start = timer()\n",
        "    for i in range(num_threads + 1):\n",
        "        thread_N = sub_N\n",
        "        if sub_N_last > 0 and i == num_threads:\n",
        "            thread_N = sub_N_last\n",
        "\n",
        "        thread = MatrixMulThread(shape1, shape2, thread_N)\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "\n",
        "    # waiting all thread finish\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "    \n",
        "    # get result from each thread\n",
        "    for thread in threads:\n",
        "        for C in thread.Cs:\n",
        "            ret_C = C\n",
        "            # print(C.shape)\n",
        "\n",
        "    end = timer()\n",
        "\n",
        "    msec_TotalMatrixMul = (end - start) * 1000\n",
        "    msec_MatrixMul = msec_TotalMatrixMul / N\n",
        "    flopsPerMatrixMul = 2.0 * width_A * height_A * width_B\n",
        "    flopsGiga = (flopsPerMatrixMul * 1.0e-9) / (msec_MatrixMul / 1000.0)\n",
        "    print(\"Performance= {:.2f} GFlop/s, AVGTime= {:.3f} msec, TotalTime= {:.3f} msc \\n\"\n",
        "            .format(flopsGiga, msec_MatrixMul, msec_TotalMatrixMul))\n",
        "\n",
        "def main():\n",
        "    # condition i) A(500*500), B(500*400), N=100\n",
        "    print(\"Performing condition i: A(500*500), B(500*400), N=100\")\n",
        "    matrix_multiply((500, 500), (500, 400), 100)\n",
        "\n",
        "    # condition ii) A(50*20), B(20*50), N=5000\n",
        "    print(\"Performing condition ii: A(50*20), B(20*50), N=5000\")\n",
        "    matrix_multiply((50, 20), (20, 50), 5000)\n",
        "\n",
        "    # condition iii) A(6*4000), B(4000*9), N=1000\n",
        "    print(\"Performing condition iii: A(6*4000), B(4000*9), N=1000\")\n",
        "    matrix_multiply((6, 4000), (4000, 9), 1000)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing condition i: A(500*500), B(500*400), N=100\n",
            "Performance= 16.87 GFlop/s, AVGTime= 11.857 msec, TotalTime= 1185.732 msc \n",
            "\n",
            "Performing condition ii: A(50*20), B(20*50), N=5000\n",
            "Performance= 1.51 GFlop/s, AVGTime= 0.066 msec, TotalTime= 330.522 msc \n",
            "\n",
            "Performing condition iii: A(6*4000), B(4000*9), N=1000\n",
            "Performance= 0.59 GFlop/s, AVGTime= 0.731 msec, TotalTime= 731.225 msc \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}